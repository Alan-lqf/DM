# 数据挖掘：理论与实践

标签（空格分隔）： 笔记

---

## 课程简介
        本课程完整覆盖数据挖掘领域的各项核心技术，包括数据预处理、分类、聚类、回归、关联、推荐、集成学习、进化计算等。强调在知识的广度、深度和趣味性之间寻找最佳平衡点，在生动幽默中讲述数据挖掘的核心思想、关键技术以及一些在其它相关课程和教科书中少有涉及的重要知识点。本课程适合对大数据和数据科学感兴趣的各专业学生以及工程技术人员学习，不追求纯粹的理论推导，而是把理论与实践有机结合，让学生学到活的知识、有用的知识和真正属于自己的知识，特别是数据分析领域的研究方法和思维方式。
---
## 课程章节
### 走进数据科学：博大精深，美不胜收

* 整装待发

* 学而不思则罔 

   weka UCI(Machine Learning Repository)


* 知行合一


* 从数据到知识

* 分类问题
        混淆矩阵（列联表，模型的准确率）
        ROC曲线
        权重（犯哪种错误的影响大？）

* 聚类及其他数据挖掘问题

        关联规则，回归

* 隐私保护与并行计算
        （云计算、GPU计算、没有一个最好的算法————先用简单的）

* 迷雾重重
        幸存者偏差
        不能以偏概全

* //相关学习资源

### 数据预处理：抽丝剥茧，去伪存真
* 数据清洗

* 异常值与重复数据检测
        distance    lrd     LOF(越大，离群点的可能性越大)

* 类型转换与采样
        数据编码方法的选择可能会大大简化数据处理
        不平衡数据集 G-mean     F-measure       (用边缘点简化计算？)

* 数据描述与可视化
        数据标准化
        相关性（r=0仅表示无‘线性’关系）
        Box Plots函数（高维度数据可视化）
        Parallel Coordinates（高维度数据可视化）
        CiteSpace（文本可视化【类似于知网上的文献被引用那个图，点越大被引用次数越多】）
        Gephi（元素之间的关系用网络展示）

* 特征选择
        熵(Entropy)
        信息增益(Information Gain)【降低不确定性的幅度，幅度越大越好】"吸烟&&男女"
        分支定界(Branch and Bound)<page 45>

* 主成分分析（PCA）
        remove correlation
        ？拉格朗日乘数法（带条件的约束问题）
        ？矩阵的特征值分解

* 线性判别分析（LDA）——保留类的区分信息【分出蓝红】

    ```
    ？几个数学公式（求导？转置？特征向量？）
    LDA的局限性<page 76>
    ```


* //相关学习资源

### 从贝叶斯到决策树：意料之外，情理之中
* 贝叶斯奇幻之旅
   $$P(A|B)=\frac{P(B|A)P(A)}{P(B)}$$

* 朴素是一种美德
         NB(Native Bayes)朴素贝叶斯
         拉普拉斯平滑（分子+1，避免分子为零）<page 21,26>
         【推荐准确率从16%提升到59%】

* 数据、规则与树 

   ```
   奥卡姆的剃刀(simpler trees are generally preferred) 
   ```

* 植树造林学问大 

     ID3算法:http://blog.csdn.net/u012897374/article/details/74505024

     $$H(X)=-\sum_{i=1}^{n}{p_i}{log_2}{p_i}$$
     $$IG(S|t)=Entropy(S)-\sum_{value(T)}\frac{|S_v|}{S}Entropy(S_v) $$

     	信息增益，降低幅度越大越好

  > H:熵
  >
  > IG:Information Gain(信息增益)

```
Overfitting(过学习)→树过长
剪枝（一般在拐点处收手）
```

  $$SplitInformation(S,A)=-\sum_{i=1}^c\frac{|S_i|}{|S|}{log_2}{\frac{|S_i|}{|S|}}$$

​		  $$GainRatio(S,A)=\frac{Gain(S,A)}{SplitInformation(S,A)}$$

  > penalize惩罚项（过学习的IG除以惩罚项后会变得很小，说明并不好） 

```
“最优切分点一定在YES和NO之间”
```

* //相关学习资源


### 神经网络：巨量并行，智慧无限

* 智慧之源神经元

```

```



会学习的神经元
从一个到一群
层次分明，责任到人
管中窥豹，抛砖引玉
相关学习资源



### 支持向量机：数学之美，巅峰之作

最大间隔
线性SVM
数学家的把戏
致敬真神
相关学习资源



### 聚类分析：物以类聚，人以群分

无监督学习
K-Means
期望最大法
密度与层次
相关学习资源



### 关联规则：营销购物，自有乾坤

项集与规则
支持度与置信度
误区

//Apriori 算法
	频繁项集的非空子集都必须是频繁的
	一个不频繁，其任何超集一定不频繁
	（循环扫描数据库得到频繁项）

实例分析
序列模式
相关学习资源



### 推荐算法：察言观色，投其所好

无所不在的推荐
隐含语义分析
PageRank传奇
协同过滤
告诉你一个真实的推荐

### 集成学习：兼听则明，偏听则暗

民主协商：Ensemble
群策群议：Bagging
环环相扣：Boosting
集成之美：AdaBoost
继往开来：RegionBoost
相关学习资源

### 进化计算：大道至简，万物之本

人与自然
尽善尽美
走向进化
遗传算法初探
遗传算法进阶
遗传程序设计
万物皆进化
相关学习资源

### 美丽数据说：阆苑仙葩，美玉无瑕

数据之美
南国紫荆亦芬芳



